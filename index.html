<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <title>Loiacono streaming WebGPU</title>
  <style>
    body {
      font-family: system-ui, sans-serif;
      margin: 1.5rem;
      color: #111;
      background: #f6f6f6;
    }

    h1 {
      font-size: 1.6rem;
    }

    .controls {
      display: flex;
      gap: 0.75rem;
      align-items: center;
      margin-bottom: 1rem;
    }

    button {
      padding: 0.5rem 0.9rem;
      font-size: 1rem;
      border-radius: 0.35rem;
      border: 1px solid #666;
      background: white;
      cursor: pointer;
    }

    button:disabled {
      opacity: 0.3;
      cursor: not-allowed;
    }

    .file-upload {
      display: flex;
      gap: 0.4rem;
      align-items: center;
      font-size: 0.9rem;
      color: #333;
    }

    .file-upload input {
      cursor: pointer;
    }

    .toggle-control {
      display: inline-flex;
      align-items: center;
      gap: 0.3rem;
      font-size: 0.9rem;
      color: #222;
      margin-left: 0.5rem;
    }

    .toggle-control input {
      cursor: pointer;
    }

    canvas {
      display: block;
      margin-top: 1rem;
      border-radius: 0.35rem;
      border: 1px solid #ccc;
      background: #111;
    }

    #status {
      font-weight: bold;
    }

    #stats {
      margin-top: 0.75rem;
      font-size: 0.9rem;
      color: #444;
      background: white;
      border-radius: 0.35rem;
      padding: 0.5rem;
      border: 1px solid #ddd;
    }
  </style>
</head>

<body>
  <h1>Streaming Loiacono Spectrogram (WebGPU)</h1>
  <div class="controls">
    <button id="startBtn">Start streaming</button>
    <button id="stopBtn" disabled>Stop</button>
    <label class="file-upload">
      Upload audio file
      <input id="audioFileInput" type="file" accept="audio/*" />
    </label>
    <label class="toggle-control">
      <input type="checkbox" id="singleChunkMode" checked />
      Single-chunk queue (debug)
    </label>
    <label class="toggle-control">
      <input type="checkbox" id="pixelPerfectMode" checked />
      Pixel-perfect (1:1) rendering
    </label>
    <span id="status">idle</span>
  </div>
  <canvas id="spectrumCanvas" width="900" height="320"></canvas>
  <pre id="stats">waiting for audio...</pre>

  <script type="module">
    const SIGNAL_LENGTH = 8192;
    const NUM_BINS = 512;
    const CHUNK_SIZE = 256;
    const WORKGROUP_SIZE = 64;
    const MULTIPLE = 15.0;
    const MIN_FREQ = 100.0;
    const MAX_FREQ = 12000.0;
    const DEFAULT_SR = 48000;
    let sampleRate = DEFAULT_SR;

    const statusEl = document.getElementById("status");
    const statsEl = document.getElementById("stats");
    const canvas = document.getElementById("spectrumCanvas");
    const singleChunkCheckbox = document.getElementById("singleChunkMode");
    const pixelPerfectCheckbox = document.getElementById("pixelPerfectMode");
    let width = canvas.width;
    let height = canvas.height;
    
    // Spectrogram parameters
    let SPECTROGRAM_WIDTH = width;
    const SPECTROGRAM_HEIGHT = NUM_BINS;
    let spectrogramHistory = [];
    let MAX_HISTORY = Math.max(1, Math.floor(width)); // One column per pixel
    let spectrogramImageData = null;
    let spectrogramCanvas = null;
    let spectrogramCtx = null;
    // Reusable 1-px column buffer to avoid per-pixel drawing
    let columnImageData = null;
    // rAF scheduling helpers
    let drawPending = false;
    let pendingSpectrum = null;
    let pendingGlobalMax = 1;
    
    function syncCanvasSize() {
      const rect = canvas.getBoundingClientRect();
      const targetWidth = Math.max(1, Math.floor(rect.width));
      const targetHeight = Math.max(1, Math.floor(rect.height));
      if (pixelPerfectCheckbox && pixelPerfectCheckbox.checked) {
        canvas.style.width = `${targetWidth}px`;
        canvas.style.height = `${targetHeight}px`;
      } else {
        canvas.style.width = "";
        canvas.style.height = "";
      }
      if (canvas.width !== targetWidth || canvas.height !== targetHeight) {
        canvas.width = targetWidth;
        canvas.height = targetHeight;
      }
      width = canvas.width;
      height = canvas.height;
      SPECTROGRAM_WIDTH = width;
      MAX_HISTORY = Math.max(1, Math.floor(width));
    }

    // Initialize spectrogram
    function initSpectrogram() {
      spectrogramHistory = [];
      spectrogramCanvas = document.createElement('canvas');
      spectrogramCanvas.width = SPECTROGRAM_WIDTH;
      spectrogramCanvas.height = SPECTROGRAM_HEIGHT;
      spectrogramCtx = spectrogramCanvas.getContext('2d');
      spectrogramImageData = spectrogramCtx.createImageData(SPECTROGRAM_WIDTH, SPECTROGRAM_HEIGHT);
      // Fill with black
      for (let i = 0; i < spectrogramImageData.data.length; i += 4) {
        spectrogramImageData.data[i] = 0;     // R
        spectrogramImageData.data[i + 1] = 0; // G
        spectrogramImageData.data[i + 2] = 0; // B
        spectrogramImageData.data[i + 3] = 255; // A
      }
      spectrogramCtx.putImageData(spectrogramImageData, 0, 0);

      // Create a reusable 1-px column buffer for fast updates
      columnImageData = spectrogramCtx.createImageData(1, SPECTROGRAM_HEIGHT);
      for (let i = 0; i < columnImageData.data.length; i += 4) {
        columnImageData.data[i] = 0;
        columnImageData.data[i + 1] = 0;
        columnImageData.data[i + 2] = 0;
        columnImageData.data[i + 3] = 255;
      }
    }
    
    // Viridis colormap (simplified)
    function viridisColor(t) {
      // t should be between 0 and 1
      const t2 = t * t;
      const t3 = t2 * t;
      // Simplified viridis approximation
      const r = Math.min(255, Math.max(0, Math.floor(255 * (0.2627 + t * 1.8811 - t2 * 2.8294 + t3 * 2.4889))));
      const g = Math.min(255, Math.max(0, Math.floor(255 * (0.1949 + t * 2.0312 - t2 * 3.3897 + t3 * 2.8606))));
      const b = Math.min(255, Math.max(0, Math.floor(255 * (0.3484 + t * 3.5947 - t2 * 6.2286 + t3 * 4.6983))));
      return [r, g, b];
    }
    
    // Convert magnitude to color using log scaling
    function magnitudeToColor(magnitude, maxMagnitude) {
      if (magnitude <= 0) return [0, 0, 0];
      // Log scale for better dynamic range
      const logVal = Math.log10(magnitude + 1) / Math.log10(maxMagnitude + 1);
      return viridisColor(Math.min(1, logVal));
    }

    let device;
    let queue;
    let pipeline;
    let bindGroup;
    // WebGPU canvas renderer resources
    let canvasGPU = null;
    let presentationFormat = null;
    let renderPipeline = null;
    let renderBindGroup = null;
    let spectrogramTexture = null;
    let spectrogramSampler = null;
    // Staging buffer for a single column (padded to bytesPerRow alignment)
    let columnBytesPerRow = 0;
    let columnBufferSize = 0;
    let columnStagingBuffer = null;
    let columnCpuView = null;

    let chunkBuffer;
    let chunkRealBuffer;
    let chunkImagBuffer;
    let freqBuffer;
    let paramsBuffer;
    let stagingBuffer;

    let adapter;
    let audioCtx;
    let audioNode;
    let mediaStream;
    let running = false;

    let pendingChunk = new Float32Array(CHUNK_SIZE);
    let pendingCount = 0;
    const WINDOW_CHUNKS = SIGNAL_LENGTH / CHUNK_SIZE;
    let processedChunks = 0;
    let totalSamplesCaptured = 0;
    const chunkQueue = [];
    let queueProcessing = false;
    let currentReal = new Float32Array(NUM_BINS);
    let currentImag = new Float32Array(NUM_BINS);
    const chunkHistory = Array.from({ length: WINDOW_CHUNKS }, () => ({
      real: new Float32Array(NUM_BINS),
      imag: new Float32Array(NUM_BINS),
    }));

    function getChunkQueueLimit() {
      if (singleChunkCheckbox && singleChunkCheckbox.checked) {
        return 1;
      }
      return Math.max(8, WINDOW_CHUNKS * 2);
    }

    window.addEventListener("resize", () => {
      syncCanvasSize();
      initSpectrogram();
      resetHistory();
    });

    const shaderPath = "shaders/loiacono_stream.wgsl";
    const fileInput = document.getElementById("audioFileInput");
    fileInput.addEventListener("change", handleFileUpload);
    singleChunkCheckbox.addEventListener("change", () => {
      chunkQueue.length = 0;
    });
    pixelPerfectCheckbox.addEventListener("change", () => {
      syncCanvasSize();
      initSpectrogram();
      resetHistory();
    });

    document.getElementById("startBtn").addEventListener("click", startStreaming);
    document.getElementById("stopBtn").addEventListener("click", stopStreaming);

    function resetHistory() {
      currentReal.fill(0);
      currentImag.fill(0);
      chunkHistory.forEach((entry) => {
        entry.real.fill(0);
        entry.imag.fill(0);
      });
      processedChunks = 0;
      totalSamplesCaptured = 0;
      chunkQueue.length = 0;
      queueProcessing = false;
      pendingChunk.fill(0);
      pendingCount = 0;
    }

    async function startStreaming() {
      if (running) {
        return;
      }
      running = true;
      document.getElementById("startBtn").disabled = true;
      document.getElementById("stopBtn").disabled = false;
      statusEl.textContent = "initializing GPU & microphone...";

      resetHistory();
      try {
        await ensureGpu();
        await ensureAudio();
        statusEl.textContent = "listening to microphone…";
        statsEl.textContent = "collecting first chunk...";
      } catch (error) {
        statusEl.textContent = `error: ${error.message}`;
        running = false;
        document.getElementById("startBtn").disabled = false;
        document.getElementById("stopBtn").disabled = true;
      }
    }

    async function stopStreaming() {
      running = false;
      document.getElementById("startBtn").disabled = false;
      document.getElementById("stopBtn").disabled = true;
      statusEl.textContent = "stopped";
      statsEl.textContent = "idle";
      if (audioNode) {
        audioNode.disconnect();
        // Clear event handlers
        if (audioNode.onaudioprocess) {
          audioNode.onaudioprocess = null;
        }
        if (audioNode.port && audioNode.port.onmessage) {
          audioNode.port.onmessage = null;
        }
        audioNode = null;
      }
      if (audioCtx) {
        audioCtx.close();
        audioCtx = null;
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach((track) => track.stop());
        mediaStream = null;
      }
      pendingCount = 0;
    }

    async function handleFileUpload(event) {
      const file = event.target?.files?.[0];
      if (!file) {
        return;
      }
      if (running) {
        stopStreaming();
      }
      try {
        await ensureGpu();
        await decodeAndStreamAudio(await file.arrayBuffer(), file.name);
      } catch (error) {
        statusEl.textContent = `file error: ${error.message}`;
        console.error(error);
      }
    }

    function mixToMono(buffer) {
      const channels = buffer.numberOfChannels;
      const length = buffer.length;
      const output = new Float32Array(length);
      for (let c = 0; c < channels; c++) {
        const channelData = buffer.getChannelData(c);
        for (let i = 0; i < length; i++) {
          output[i] += channelData[i];
        }
      }
      if (channels > 1) {
        const factor = 1 / channels;
        for (let i = 0; i < length; i++) {
          output[i] *= factor;
        }
      }
      return output;
    }

    async function streamUploadedSamples(samples, options = {}) {
      const { singleChunkMode = false } = options;
      statsEl.textContent = `playing uploaded audio (${samples.length} samples)…`;
      let offset = 0;
      while (offset < samples.length) {
        const end = Math.min(samples.length, offset + CHUNK_SIZE);
        feedSamples(samples.subarray(offset, end));
        offset = end;
        if (singleChunkMode) {
          await waitForQueueIdle();
        } else {
          await new Promise((resolve) => setTimeout(resolve, 0));
        }
      }
      if (pendingCount > 0) {
        const padding = new Float32Array(CHUNK_SIZE - pendingCount);
        feedSamples(padding);
      }
      await waitForQueueIdle();
    }

    async function decodeAndStreamAudio(arrayBuffer, label) {
      statusEl.textContent = `decoding ${label}…`;
      const decodeCtx = new AudioContext();
      const audioBuffer = await decodeCtx.decodeAudioData(arrayBuffer);
      await decodeCtx.close();
      if (audioBuffer.length === 0) {
        throw new Error("decoded buffer is empty");
      }
      if (audioBuffer.sampleRate !== sampleRate) {
        sampleRate = audioBuffer.sampleRate;
        updateFrequencyBuffer(sampleRate);
      }
      resetHistory();
      await streamUploadedSamples(mixToMono(audioBuffer), {
        singleChunkMode: singleChunkCheckbox?.checked,
      });
      statusEl.textContent = `processed ${label}`;
    }

    async function ensureGpu() {
      if (device) {
        return;
      }
      if (!navigator.gpu) {
        throw new Error("WebGPU is not supported in this browser. Please use Chrome 113+, Edge 113+, or Safari 17+ with WebGPU enabled.");
      }
      
      let groupOpen = false;
      try {
        console.groupCollapsed("WebGPU init");
        groupOpen = true;
        console.log("navigator.gpu available:", !!navigator.gpu);
        adapter = await navigator.gpu.requestAdapter();
        console.log("requested adapter:", adapter);
        if (!adapter) {
          throw new Error("Failed to request a GPU adapter. Your system may not have a compatible GPU or GPU drivers.");
        }
        device = await adapter.requestDevice();
        console.log("device acquired");
        queue = device.queue;

        // Configure canvas for WebGPU presentation
        // Modern browsers use 'webgpu' context
        const rect = canvas.getBoundingClientRect();
        console.log("canvas metrics", { width: canvas.width, height: canvas.height, rect });
        canvasGPU = canvas.getContext('webgpu');
        console.log("webgpu context", canvasGPU);
        if (!canvasGPU) {
          // Fallback for older implementations (though unlikely to work)
          canvasGPU = canvas.getContext('gpupresent');
          console.log("gpupresent context", canvasGPU);
        }
        if (!canvasGPU) {
          throw new Error("Failed to acquire a WebGPU canvas context. The canvas element may not be ready or WebGPU canvas support is not available.");
        }
        presentationFormat = navigator.gpu.getPreferredCanvasFormat();
        canvasGPU.configure({ 
          device, 
          format: presentationFormat, 
          alphaMode: 'opaque',
          usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.COPY_SRC
        });

        syncCanvasSize();
        initSpectrogram(); // Initialize spectrogram (CPU-side structures)
        await setupPipeline();
        // Create GPU-side spectrogram resources (texture / staging buffer / bind group)
        createSpectrogramResources();
        
        console.log("WebGPU initialized successfully");
      } catch (error) {
        // Reset state so future calls will retry
        device = null;
        adapter = null;
        canvasGPU = null;
        throw error;
      } finally {
        if (groupOpen) {
          console.groupEnd();
        }
      }
    }

    async function setupPipeline() {
      const shaderReq = await fetch(shaderPath);
      if (!shaderReq.ok) {
        throw new Error(`Failed to load shader: ${shaderPath} (${shaderReq.status})`);
      }
      const shaderText = await shaderReq.text();
      const module = device.createShaderModule({ code: shaderText });
      pipeline = device.createComputePipeline({
        layout: "auto",
        compute: { module, entryPoint: "main" },
      });

      chunkBuffer = device.createBuffer({
        size: CHUNK_SIZE * 4,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });

      chunkRealBuffer = device.createBuffer({
        size: NUM_BINS * 4,
        usage:
          GPUBufferUsage.STORAGE |
          GPUBufferUsage.COPY_SRC |
          GPUBufferUsage.COPY_DST,
      });
      chunkImagBuffer = device.createBuffer({
        size: NUM_BINS * 4,
        usage:
          GPUBufferUsage.STORAGE |
          GPUBufferUsage.COPY_SRC |
          GPUBufferUsage.COPY_DST,
      });
      freqBuffer = device.createBuffer({
        size: NUM_BINS * 4,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });
      paramsBuffer = device.createBuffer({
        size: 16,
        usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
      });
      stagingBuffer = device.createBuffer({
        size: NUM_BINS * 4 * 2,
        usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ,
      });

      const freqs = buildFrequencyArray(sampleRate);
      queue.writeBuffer(freqBuffer, 0, freqs.buffer, freqs.byteOffset, freqs.byteLength);

      bindGroup = device.createBindGroup({
        layout: pipeline.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: { buffer: chunkBuffer } },
          { binding: 1, resource: { buffer: chunkRealBuffer } },
          { binding: 2, resource: { buffer: chunkImagBuffer } },
          { binding: 3, resource: { buffer: freqBuffer } },
          { binding: 4, resource: { buffer: paramsBuffer } },
        ],
      });

      // Create a small textured fullscreen render pipeline to draw the spectrogram texture
      const vsCode = `
        @vertex
        fn vs(@builtin(vertex_index) v : u32) -> @builtin(position) vec4<f32> {
          var pos = array<vec2<f32>, 6>(
            vec2<f32>(-1.0, -1.0), vec2<f32>(1.0, -1.0), vec2<f32>(-1.0, 1.0),
            vec2<f32>(-1.0, 1.0), vec2<f32>(1.0, -1.0), vec2<f32>(1.0, 1.0)
          );
          return vec4<f32>(pos[v], 0.0, 1.0);
        }
      `;
      const fsCode = `
        @group(0) @binding(0) var samp : sampler;
        @group(0) @binding(1) var tex : texture_2d<f32>;
        @fragment
        fn fs(@builtin(position) fragCoord: vec4<f32>) -> @location(0) vec4<f32> {
          // Map fragCoord.xy (clip space) to UV (0..1)
          let uv = (fragCoord.xy * 0.5 + vec2<f32>(0.5, 0.5));
          // Flip Y to match texture coordinate where 0 is top
          let color = textureSample(tex, samp, vec2<f32>(uv.x, 1.0 - uv.y));
          return color;
        }
      `;
      const vsModule = device.createShaderModule({ code: vsCode });
      const fsModule = device.createShaderModule({ code: fsCode });

      renderPipeline = device.createRenderPipeline({
        layout: 'auto',
        vertex: { module: vsModule, entryPoint: 'vs' },
        fragment: {
          module: fsModule,
          entryPoint: 'fs',
          targets: [{ format: presentationFormat }],
        },
        primitive: { topology: 'triangle-list' },
      });
    }

    // Create GPU-side spectrogram texture and staging buffers. This will be called after pipeline setup and whenever the canvas/width changes.
    function createSpectrogramResources() {
      if (!device) return;
      // Destroying textures isn't necessary; just recreate and rebind resources
      // Create spectrogram texture (RGBA8) that we will copy into and sample from
      spectrogramTexture = device.createTexture({
        size: { width: Math.max(1, SPECTROGRAM_WIDTH), height: SPECTROGRAM_HEIGHT },
        format: 'rgba8unorm',
        usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.COPY_SRC | GPUTextureUsage.RENDER_ATTACHMENT,
      });
      spectrogramSampler = device.createSampler({ magFilter: 'nearest', minFilter: 'nearest' });

      // Prepare a padded buffer for buffer->texture copies. bytesPerRow must be a multiple of 256.
      const singleRowBytes = 4 * 1; // rgba8 per pixel * width (1)
      const alignment = 256;
      columnBytesPerRow = Math.ceil(singleRowBytes / alignment) * alignment;
      columnBufferSize = columnBytesPerRow * SPECTROGRAM_HEIGHT;

      columnStagingBuffer = device.createBuffer({ size: columnBufferSize, usage: GPUBufferUsage.COPY_SRC | GPUBufferUsage.MAP_WRITE });
      columnCpuView = new Uint8Array(columnBufferSize);

      // Create render bind group for sampling the spectrogram texture
      if (renderPipeline) {
        renderBindGroup = device.createBindGroup({
          layout: renderPipeline.getBindGroupLayout(0),
          entries: [
            { binding: 0, resource: spectrogramSampler },
            { binding: 1, resource: spectrogramTexture.createView() },
          ],
        });
      }
    }

    async function ensureAudio() {
      audioCtx = new AudioContext();
      sampleRate = audioCtx.sampleRate;
      updateFrequencyBuffer(sampleRate);
      
      // Try to use AudioWorklet if supported, fall back to ScriptProcessorNode
      if (audioCtx.audioWorklet && typeof AudioWorkletNode === 'function') {
        try {
          // Load and add the audio worklet module
          await audioCtx.audioWorklet.addModule('audio-processor.js');
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          const source = audioCtx.createMediaStreamSource(mediaStream);
          audioNode = new AudioWorkletNode(audioCtx, 'audio-stream-processor');
          
          audioNode.port.onmessage = (event) => {
            if (event.data.type === 'audioData') {
              feedSamples(event.data.data);
            }
          };
          
          source.connect(audioNode);
          audioNode.connect(audioCtx.destination);
          return;
        } catch (error) {
          console.warn('AudioWorklet failed, falling back to ScriptProcessorNode:', error);
          // Fall through to ScriptProcessorNode implementation
        }
      }
      
      // Fallback to ScriptProcessorNode (deprecated but works)
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const source = audioCtx.createMediaStreamSource(mediaStream);
      audioNode = audioCtx.createScriptProcessor(512, 1, 1);
      source.connect(audioNode);
      audioNode.connect(audioCtx.destination);
      audioNode.onaudioprocess = (event) => {
        const input = event.inputBuffer.getChannelData(0);
        feedSamples(input);
      };
    }

    function feedSamples(samples) {
      let offset = 0;
      while (offset < samples.length) {
        const need = CHUNK_SIZE - pendingCount;
        const copyLen = Math.min(need, samples.length - offset);
        pendingChunk.set(samples.subarray(offset, offset + copyLen), pendingCount);
        pendingCount += copyLen;
        offset += copyLen;
        totalSamplesCaptured += copyLen;

        if (pendingCount === CHUNK_SIZE) {
          const chunkData = pendingChunk.slice();
          const chunkStart = Math.max(0, totalSamplesCaptured - CHUNK_SIZE);
          chunkQueue.push({ data: chunkData, start: chunkStart });
          console.debug("queued chunk", chunkQueue.length, chunkStart);
          const queueLimit = getChunkQueueLimit();
          if (chunkQueue.length > queueLimit) {
            chunkQueue.shift();
          }
          pendingCount = 0;
          processChunkQueue();
        }
      }
    }

    function processChunkQueue() {
      if (queueProcessing) {
        return;
      }
      queueProcessing = true;
      (async () => {
        try {
          while (chunkQueue.length > 0) {
            const { data, start } = chunkQueue.shift();
            console.debug("dispatch chunk", start, "queue len", chunkQueue.length);
            await runChunk(data, CHUNK_SIZE, start, processedChunks);
            processedChunks += 1;
          }
        } finally {
          queueProcessing = false;
        }
      })();
    }

    function waitForQueueIdle() {
      return new Promise((resolve) => {
        const check = () => {
          if (!queueProcessing && chunkQueue.length === 0) {
            resolve();
          } else {
            setTimeout(check, 10);
          }
        };
        check();
      });
    }

    async function runChunk(chunkData, length, chunkStart, chunkIndex) {
      // Validate that WebGPU resources are initialized
      if (!device || !queue || !chunkBuffer || !paramsBuffer || !pipeline || !bindGroup) {
        console.error('WebGPU resources not initialized. Skipping chunk processing.');
        return;
      }
      
      try {
        queue.writeBuffer(chunkBuffer, 0, chunkData.buffer, 0, CHUNK_SIZE * 4);
        const params = new Uint32Array([chunkStart >>> 0, length, 0, 0]);
        queue.writeBuffer(paramsBuffer, 0, params.buffer, params.byteOffset, params.byteLength);

        const encoder = device.createCommandEncoder();
        const pass = encoder.beginComputePass();
        pass.setPipeline(pipeline);
        pass.setBindGroup(0, bindGroup);
        pass.dispatchWorkgroups(Math.ceil(NUM_BINS / WORKGROUP_SIZE));
        pass.end();

        encoder.copyBufferToBuffer(
          chunkRealBuffer,
          0,
          stagingBuffer,
          0,
          NUM_BINS * 4
        );
        encoder.copyBufferToBuffer(
          chunkImagBuffer,
          0,
          stagingBuffer,
          NUM_BINS * 4,
          NUM_BINS * 4
        );
        queue.submit([encoder.finish()]);

        await stagingBuffer.mapAsync(GPUMapMode.READ);
        const mapped = stagingBuffer.getMappedRange(0, NUM_BINS * 4 * 2);
        const realChunk = new Float32Array(mapped, 0, NUM_BINS);
        const imagChunk = new Float32Array(
          mapped,
          NUM_BINS * 4,
          NUM_BINS
        );
        const realData = new Float32Array(realChunk);
        const imagData = new Float32Array(imagChunk);
        stagingBuffer.unmap();

        const historySlot = chunkHistory[chunkIndex % WINDOW_CHUNKS];
        if (chunkIndex >= WINDOW_CHUNKS) {
          const oldReal = historySlot.real;
          const oldImag = historySlot.imag;
          for (let i = 0; i < NUM_BINS; ++i) {
            currentReal[i] -= oldReal[i];
            currentImag[i] -= oldImag[i];
          }
        }
        historySlot.real.set(realData);
        historySlot.imag.set(imagData);
        for (let i = 0; i < NUM_BINS; ++i) {
          currentReal[i] += realData[i];
          currentImag[i] += imagData[i];
        }

        const magnitudes = new Float32Array(NUM_BINS);
        for (let i = 0; i < NUM_BINS; ++i) {
          magnitudes[i] = Math.sqrt(
            currentReal[i] * currentReal[i] + currentImag[i] * currentImag[i]
          );
        }
        drawSpectrum(magnitudes, chunkIndex);
      } catch (error) {
        console.error('Error in runChunk:', error);
        // Don't rethrow to avoid breaking the processing loop
      }
    }

    function drawSpectrum(data, chunkIndex) {
      // Add new spectrum to history
      spectrogramHistory.push(data);
      if (spectrogramHistory.length > MAX_HISTORY) {
        spectrogramHistory.shift(); // Remove oldest
      }

      // Find global maximum for color mapping
      let globalMax = 0;
      for (const spectrum of spectrogramHistory) {
        const maxInSpectrum = Math.max(...spectrum);
        if (maxInSpectrum > globalMax) {
          globalMax = maxInSpectrum;
        }
      }

      // Store pending spectrum and schedule a draw on the next animation frame.
      pendingSpectrum = spectrogramHistory[spectrogramHistory.length - 1];
      pendingGlobalMax = globalMax || 1;

      if (!drawPending) {
        drawPending = true;
        requestAnimationFrame(function flushSpectrogram() {
          drawPending = false;

            // Build the padded column buffer (RGBA8) directly into the CPU staging view
          if (!columnCpuView) {
            console.warn('columnCpuView not ready; skipping frame');
          } else {
            for (let y = 0; y < SPECTROGRAM_HEIGHT; y++) {
              const binIndex = SPECTROGRAM_HEIGHT - 1 - y;
              const magnitude = pendingSpectrum[binIndex];
              const [r, g, b] = magnitudeToColor(magnitude, pendingGlobalMax);
              const base = y * columnBytesPerRow;
              columnCpuView[base] = r;
              columnCpuView[base + 1] = g;
              columnCpuView[base + 2] = b;
              columnCpuView[base + 3] = 255;
              // zeros for padding bytes are already present or not needed
            }

            // Write staging buffer then issue GPU-side copy & render commands in one encoder
            const encoder = device.createCommandEncoder();

            // Shift texture contents right by 1 pixel (src x=0 -> dst x=1)
            if (SPECTROGRAM_WIDTH > 1) {
              encoder.copyTextureToTexture(
                { texture: spectrogramTexture, origin: { x: 0, y: 0 } },
                { texture: spectrogramTexture, origin: { x: 1, y: 0 } },
                { width: Math.max(0, SPECTROGRAM_WIDTH - 1), height: SPECTROGRAM_HEIGHT, depthOrArrayLayers: 1 }
              );
            }

            // Upload the leftmost column via a padded buffer -> texture copy (bytesPerRow must be aligned)
            // Use queue.writeBuffer for simplicity (ensures the buffer contents are current before the copy)
            queue.writeBuffer(columnStagingBuffer, 0, columnCpuView);
            encoder.copyBufferToTexture(
              { buffer: columnStagingBuffer, bytesPerRow: columnBytesPerRow },
              { texture: spectrogramTexture, origin: { x: 0, y: 0 } },
              { width: 1, height: SPECTROGRAM_HEIGHT, depthOrArrayLayers: 1 }
            );

            // Render the texture to the canvas via render pass
            const swapView = canvasGPU.getCurrentTexture().createView();
            const pass = encoder.beginRenderPass({
              colorAttachments: [
                {
                  view: swapView,
                  loadOp: 'clear',
                  clearValue: { r: 0, g: 0, b: 0, a: 1 },
                  storeOp: 'store',
                },
              ],
            });
            pass.setPipeline(renderPipeline);
            pass.setBindGroup(0, renderBindGroup);
            pass.draw(6);
            pass.end();

            queue.submit([encoder.finish()]);

            // Update stats with magnitude info
            const currentMax = Math.max(...pendingSpectrum);
            statsEl.textContent = `Spectrogram: ${spectrogramHistory.length} cols, chunk #${chunkIndex}, Global max: ${pendingGlobalMax.toFixed(4)}, Current max: ${currentMax.toFixed(4)}`;
          }
        });
      }

      console.debug("queued chunk", chunkIndex, "max", Math.max(...pendingSpectrum));
    }

    async function autoplayChirp() {
      try {
        await ensureGpu();
        const response = await fetch("chirp.wav");
        if (!response.ok) {
          throw new Error("Failed to fetch chirp.wav");
        }
        console.debug("chirp fetch status", response.status);
        await decodeAndStreamAudio(await response.arrayBuffer(), "chirp.wav");
      } catch (error) {
        console.warn("Chirp auto-run failed:", error);
        // Update status to show the error to user
        statusEl.textContent = `Auto-run failed: ${error.message}`;
        statsEl.textContent = "Check browser console for details";
      }
    }

    // Wait for DOM to be ready before attempting WebGPU initialization
    document.addEventListener('DOMContentLoaded', () => {
      // Initialize canvas size and spectrogram
      syncCanvasSize();
      initSpectrogram();
      
      // Try autoplay after a short delay to ensure everything is ready
      setTimeout(() => {
        autoplayChirp();
      }, 100);
    });

    function buildFrequencyArray(rate) {
      const freqs = new Float32Array(NUM_BINS);
      const minNorm = MIN_FREQ / rate;
      const maxNorm = MAX_FREQ / rate;
      const logMin = Math.log(minNorm);
      const logMax = Math.log(maxNorm);
      const delta = logMax - logMin;
      for (let i = 0; i < NUM_BINS; ++i) {
        const ratio = i / (NUM_BINS - 1);
        freqs[i] = Math.exp(logMin + delta * ratio);
      }
      return freqs;
    }

    function updateFrequencyBuffer(rate) {
      if (!freqBuffer) {
        return;
      }
      const freqs = buildFrequencyArray(rate);
      queue.writeBuffer(freqBuffer, 0, freqs.buffer, freqs.byteOffset, freqs.byteLength);
    }
  </script>
</body>

</html>
